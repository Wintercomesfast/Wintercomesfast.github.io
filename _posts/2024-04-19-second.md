---
layout: single
title:  "2024. 04. 19 Intern Study 2주차"
categories: winter's coding
tag: [python, pytorch]
toc: true
author_profile: false
---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3; 
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 0.8rem !important;
  }

  </style>
</head>


- 2024년 04월 19일 13:00 인턴 커리큘럼 2주차

- 파이토치 기본 구조 이해 및 Custom Dataset Load

- test set 성능 80퍼 이상 구현

- MNIST, CIFAR-10



```python
# 파이토치 기본 세팅 (Custom dataset Load)

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models
from torch.utils.data import DataLoader
from tqdm import tqdm
```

1) MNIST



```python
class MNISTDataset(Dataset):
    def __init__(self, train=True, transform=None):
        self.train = train
        self.transform = transform
        
        if self.train:
            self.data = datasets.MNIST(root='data', train=True, download=True, transform=self.transform)
        else:
            self.data = datasets.MNIST(root='data', train=False, download=True, transform=self.transform)
            
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, index):
        image, label = self.data[index]
        return image, label
```


```python
# transform
# MNIST dataset 평균 0.1307 표준편차 0.3081 / 걍 0.5 때려박은 것보다 확실히 acc이 높아지네..

mnist_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])
```


```python
# train, test dataset

mnist_train_dataset = MNISTDataset(train=True, transform=mnist_transform)
mnist_test_dataset = MNISTDataset(train=False, transform=mnist_transform)
```


```python
# DataLoader

mnist_train_loader = DataLoader(mnist_train_dataset, batch_size=64, shuffle=True)
mnist_test_loader = DataLoader(mnist_test_dataset, batch_size=64, shuffle=False)
```


```python
for images, labels in mnist_train_loader:
    pass
```


```python
# loss function으로 CrossEntropy 사용할 거임.
criterion = nn.CrossEntropyLoss()

# optimizer는 Adam 쓸 거임 왜냐? 박사보이가 이게 제일 좋다고 했으니까.
optimizer = optim.Adam(model.parameters(), lr=0.001) # 박사보이가 lr 1e-3 이나 1e-4로 하는 게 좋다고 했음
```


```python
# ResNet

resnet18 = models.resnet18(weights=models.ResNet18_Weights.DEFAULT) # 그래서 pretrained 왜 안 된다는 거임?
resnet18.fc = nn.Linear(resnet18.fc.in_features, 10)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # 난.. 어차피 cpu인데..
resnet18 = resnet18.to(device)
```

<pre>
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /Users/wintercomesfast/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
52.5%IOPub message rate exceeded.
The Jupyter server will temporarily stop sending output
to the client in order to avoid crashing it.
To change this limit, set the config variable
`--ServerApp.iopub_msg_rate_limit`.

Current values:
ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)
ServerApp.rate_limit_window=3.0 (secs)

100.0%
</pre>

```python
# 모델 학습
# epoch = 전체 데이터에 대해 한번 돌리는 게 에포크 1
# running loss, optimizer gradient 초기화


def train_mnist(model, epochs, lr):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        correct = 0
        total = 0

        for images, labels in tqdm(mnist_train_loader, desc="Training"):
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_accuracy = 100 * correct / total
        print(f"MNIST - Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(mnist_train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%")

        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for images, labels in tqdm(mnist_test_loader, desc="Testing"):
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        test_accuracy = 100 * correct / total
        print(f"MNIST - Test Accuracy: {test_accuracy:.2f}%")

        if test_accuracy >= 80.0:
            print("80 넘었다") # 걍 80 넘을 때까지 돌려버리기
            break

# ResNet18 모델 학습 (MNIST)
train_mnist(resnet18, epochs=5, lr=0.001) #에포크를 늘리든지.. 러닝레이트 줄이든지..
```

<pre>
Training:   0%|                                                                 | 0/938 [00:00<?, ?it/s]
</pre>
2. CIFAR-10



```python
# CIFAR-10 Dataset       

class CIFAR10Dataset(Dataset):
    def __init__(self, train=True, transform=None):
        self.train = train
        self.transform = transform
        
        if self.train:
            self.data = datasets.CIFAR10(root='data', train=True, download=True, transform=self.transform)
        else:
            self.data = datasets.CIFAR10(root='data', train=False, download=True, transform=self.transform)
            
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, index):
        image, label = self.data[index]
        return image, label
```


```python
# transform
# CIFAR dataset RGB 채널 평균: (0.4914, 0.4822, 0.4465) 표준편차: (0.2023, 0.1994, 0.2010)

cifar10_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
]) 
```


```python
# create datasets

cifar10_train_dataset = CIFAR10Dataset(train=True, transform=cifar10_transform)
cifar10_test_dataset = CIFAR10Dataset(train=False, transform=cifar10_transform)
```


```python
# DataLoader

cifar10_train_loader = DataLoader(cifar10_train_dataset, batch_size=64, shuffle=True)
cifar10_test_loader = DataLoader(cifar10_test_dataset, batch_size=64, shuffle=False)
```


```python
for images, labels in cifar10_test_loader:
    pass
```


```python
# VGG

vgg16 = models.vgg16(weights=models.VGG16_Weights.DEFAULT) # 얘도 pretrained 안된다그러네
vgg16.classifier[-1] = nn.Linear(vgg16.classifier[-1].in_features, 10)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # 난 맥이라고
vgg16 = vgg16.to(device)
```


```python
# loss function으로 CrossEntropy 사용할 거임
criterion = nn.CrossEntropyLoss()

# optimizer는 Adam 쓸 거임 왜냐? 박사보이가 이게 제일 좋다고 했으니까
optimizer = optim.Adam(model.parameters(), lr=1e-3) # 박사보이가 lr 이렇게 하면 웬만한 거 다 잘 돌아간댔음

# 모델 학습
def train_model(model, epochs, lr):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        correct = 0
        total = 0

        for images, labels in tqdm(cifar10_train_loader):
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        train_accuracy = 100 * correct / total
        print(f"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(cifar10_train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%")

        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for images, labels in cifar10_test_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        test_accuracy = 100 * correct / total
        print(f"Test Accuracy: {test_accuracy:.2f}%")

        if test_accuracy >= 80.0:
            print("Target accuracy reached!") # 걍 80 넘을 때까지 돌려버리기
            break

train_model(vgg16, epochs=5, lr=1e-3) # 에포크 늘려야겠네..
```

<pre>
100%|█████████████████████████████████████████████████████████████████| 782/782 [23:52<00:00,  1.83s/it]
</pre>
<pre>
Epoch [1/5], Train Loss: 2.3058, Train Accuracy: 9.92%
Test Accuracy: 10.00%
</pre>
<pre>
100%|█████████████████████████████████████████████████████████████████| 782/782 [23:12<00:00,  1.78s/it]
</pre>
<pre>
Epoch [2/5], Train Loss: 2.3051, Train Accuracy: 9.85%
Test Accuracy: 10.00%
</pre>
<pre>
100%|█████████████████████████████████████████████████████████████████| 782/782 [21:38<00:00,  1.66s/it]
</pre>
<pre>
Epoch [3/5], Train Loss: 2.3042, Train Accuracy: 10.13%
Test Accuracy: 10.00%
</pre>
<pre>
100%|█████████████████████████████████████████████████████████████████| 782/782 [20:18<00:00,  1.56s/it]
</pre>
<pre>
Epoch [4/5], Train Loss: 2.3043, Train Accuracy: 9.87%
Test Accuracy: 10.00%
</pre>
<pre>
100%|█████████████████████████████████████████████████████████████████| 782/782 [19:13<00:00,  1.48s/it]
</pre>
<pre>
Epoch [5/5], Train Loss: 2.3036, Train Accuracy: 9.92%
Test Accuracy: 10.00%
</pre>
